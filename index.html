<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis">
  <meta name="keywords" content="mechanistic interpretability, neuron attribution">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://zepingyu0512.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://zepingyu0512.github.io/neuron-attribution.github.io/">
            Neuron-level knowledge attribution in LLM
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2402.02872">
            understanding in context learning mechanism in LLM
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">EMNLP 2024 (main)</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zepingyu0512.github.io/">Zeping Yu</a>,</span>
            <span class="author-block">
              <a href="https://research.manchester.ac.uk/en/persons/sophia.ananiadou">Sophia Ananiadou</a></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Manchester</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2409.14144"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2409.14144"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/zepingyu0512/arithmetic-mechanism"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Slides</span>
                  </a>
              <span class="link-block">
                <a href="https://zhuanlan.zhihu.com/p/810436692"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Zhihu</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We find arithmetic ability resides within a limited number of attention heads, with each head
            specializing in distinct operations. To delve
            into the reason, we introduce the Comparative
            Neuron Analysis (CNA) method, which identifies an internal logic chain consisting of four
            distinct stages from input to prediction: feature
            enhancing with shallow FFN neurons, feature
          transferring by shallow attention layers, feature predicting by arithmetic heads, and prediction 
            enhancing among deep FFN neurons.
            Moreover, we identify the human-interpretable
            FFN neurons within both feature-enhancing
            and feature-predicting stages. These findings
            lead us to investigate the mechanism of LoRA,
            revealing that it enhances prediction probabilities by amplifying the coefficient scores of
            FFN neurons related to predictions. Finally, we
            apply our method in model pruning for arithmetic tasks and model editing for reducing gender bias.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src=""
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="container is-max-desktop"">
    <!-- Introduction. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
          Arithmetic ability is a crucial foundational skill of large language models, which is related to reasoning ability.
          Previous studies (Stolfo et al., 2023) explored the layer-level information flow in arithmetic tasks. 
          However, <b>layer-level information flow is not enough for understanding the mechanism.</b> Many studies have found that
          the attention heads and FFN neurons are the fundamental units for storing different abilities and different knowledge.
          Furthermore, as model editing typically occurs at the neuron level, it remains unclear how to effectively leverage 
          the explanations due to the uncertainty surrounding the precise locations of important parameters.
          </p>
          <p>
          In this study, we take <b>attention heads and FFN neurons as fundamental units</b>, and explore the exact parameters store
          the arithmetic ability for different operations. We observe that only a minority of heads play significant roles in 
          arithmetic tasks, which we refer to as "arithmetic heads". Through experiments involving 1-digit to 3-digit 
          operations, we find critical memorization of 1-digit operations is lost when these heads are intervened.
          </p>
          <p>
          To explore the underlying mechanisms of this phenomenon, we propose the Comparative Neuron Analysis (CNA) method,
          which compares the change of neurons between the original model and the intervened model for the same case. 
          We construct the internal logic chain by identifying four distinct stages that span from inputs to prediction, 
          as depicted in Figure 1. During the <b>feature enhancing stage</b>, <b>hidden-interpretable features</b> are extracted from 
          shallow FFN neurons. Subsequently, in the <b>feature transferring stage</b>, shallow attention layers convert these 
          features into <b>directly interpretable features</b> and then transfer them to the last position. In the <b>feature 
          predicting stage</b>, the arithmetic heads play critical roles, activating deep FFN neurons related to the final 
          prediction. Finally, a <b>prediction enhancing stage</b> exists among deep FFN neurons. Lower FFN neurons activate upper
          FFN neurons, while both of them enhance the probability of the final prediction.
          </p>
          <p>
          Based on this analysis, we investigate the mechanism of LoRA. Employing our CNA method to compare the original 
          model with the fine-tuned LoRA model, we note a significant increase in the coefficient scores of crucial deep 
          FFN neurons. Hence, we conclude that <b>LoRA enhances the final prediction by amplifying the coefficient scores of 
          important FFN neurons</b>. Finally, using our findings, we develop methods on <b>model pruning for arithmetic tasks</b>, 
          and <b>model editing for reducing gender bias</b>.
          </p>
        </div>
        <div class="column is-centered has-text-centered">
          <img src="./static/images/arithmetic.jpg"
               class="interpolation-image"
               alt="Interpolate start reference image."/>
          <p>Neuron-Level Information Flow</p>
        </div>
        <br/>
      </div>
    </div>
    <!--/ Introduction. -->
  </div>
</section>


<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="container is-max-desktop"">
    <!-- Introduction. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Contributions</h2>
        <div class="content has-text-justified">
          <p>
          <b>To summarize, our contributions are as follows.</b>
          </p> 
          <p>
          <b>a) We find the reason why only a few heads can influence 
          arithmetic ability is that these heads store crucial parameters for memorizing 1D operations. We identify 
          human-interpretable FFN neurons across both shallow and deep layers. </b>
          </p> 
          <p>
          <b>b) We propose the CNA method and construct
          the internal logic chain from inputs to prediction with four stages: feature enhancing, feature transferring, 
          feature predicting, prediction enhancing. </b>
          </p> 
          <p>
          <b>c) We use the CNA method to explore the mechanism of LoRA and find 
          LoRA increases the probability of final predictions by amplifying the important FFN neurons’ coefficient scores. 
          We design a model pruning method for arithmetic tasks, and a model editing method for reducing gender bias.</b>
          </p>
        </div>
        <br/>
      </div>
    </div>
    <!--/ Introduction. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Mechanistic interpretability evidence -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Four stages of arithmetic tasks</h2>
        <div class="content has-text-justified">
          <p>
          We show how we find the four stages in arithmetic tasks. More experiments and analysis can be found in the paper.
          </p>
          <p>  
          <b>Step 1: find arithmetic head using causal-based method.</b> We conduct experiments on 1-digit, 2-digit and 3-digit
          arithmetic operations, and find that zero interventions on only a few heads can cause very much decrease. 
          For example, when intervening the arithmetic head 17-22 (the 22th head on layer 17), the 2-digit addition 
          accuracy drops from 96.8 to 42.9%. The 1-digit addition accuracy drops from 88.9% to 47.6%. Therefore, 
          head 17-22 stores the 1-digit addition memorization.
          </p>
          <p>  
          <b>Step 2: find deep FFN neurons using comparable neuron analysis (CNA) method.</b> The core idea of our proposed
          comparable neuron analysis method is to compare the change of the same neuron's importance scores between the 
          original model and the intervened model (replacing head 17-22 parameters with zero). If a neuron is affected much,
          its importance score should reduce much. Specifically, the importance score is log probability increase in unembedding
          space. When intervening top100 identified neurons, the accuracy drops 100%. When keeping these top100 neurons and
          intervening all the other neurons in deep layers, the accuracy only drops 3.9%. Therefore, arithmetic head plays the
          "feature predicting" role by activating the identified deep FFN neurons.
          </p>
          <p>  
          <b>Step 3: prediction enhancing stage among deep FFN neurons.</b> Among the identified top100 deep FFN neurons, we find that the
          lower FFN neurons can activate the upper FFN neurons. When zero-intervening the lowest neuron among the top100 neurons,
          the sum of all the neurons' coefficient score decrease much. Therefore, there is a prediction enhancing stage among deep 
          FFN neurons.
          </p>
          <p>    
          <b>Step 4: feature enhancing stage with shallow FFN neurons.</b> It is hard to locate the important shallow FFN neurons,
          because they usually don't contain the information related to the final predictions directly. We analyze the important
          shallow FFN neurons in case "3+5=" -> "8" by computing the inner product between the shallow neurons and the 
          important deep neurons. We find that the important shallow neurons are hidden-interpretable. When directly projecting
          these neurons in unembedding space, the top tokens are not interpretable. But if we compute these neurons' transformed
          vectors by attention layers, these transformed neurons become interpretable in unembedding space. This phenomenon is 
          because all the shallow neurons need to be transformed by attention layers before transferring into the last position.
          We design a zero-shot method to locate the hidden-interpretable neurons. We compute the upper layers’ transform of each
          neuron and project them into unembedding space. If the top tokens are related to the input number or operators, we add 
          this shallow neuron into a neuron set. At last, we mask the shallow neurons in the neuron set and compute the accuracy 
          decrease on all one-digit cases. The accuracy drops much. This can prove that the hidden-interpretable neurons are important.
          </p>
          <p> 
          <b>Constucting the internal logic chain of "3+5=" -> "8".</b> First, in shallow layers there is a feature enhancing stage, 
          the hidden-interpretable shallow FFN neurons related to the input tokens are activated. Then these enhanced features 
          are transformed into the last position. The arithmetic head takes all these enhanced features as input and activate the
          deep FFN neurons related to the final prediction eight. Among the deep neurons, the lower neurons activate the upper 
          neurons during the prediction enhancing stage.
          </p>
          <p> 
          <b>Top tokens of identified hidden-interpretable shallow FFN neurons:</b>
          </p>
          <p>
          12_4072 (not transformed): [rd, quarters, PO, Constraint, ran, avas]
          </p>
          <p>
          12_4072 (attention transformed): [III, three, Three, 3, triple]
          </p>
          <p>
          11_2258 (not transformed): [enz, Trace, lis, vid, suite, HT, ung, icano]
          </p>
          <p>
          11_2258 (attention transformed): [XV, fifth, Fif, avas, Five, five, abase, fif]
          </p>
          <p> 
          <b>Top tokens of identified shallow FFN neurons using comparable neuron analysis (CNA):</b>
          </p>
          <p>
          28_3696: [8, eight, VIII, huit, acht, otto]
          </p>
          <p>
          25_7164: [six, eight, acht, Four, twelve, six, four]
          </p>
          <p>
          19_5769: [eight, VIII, 8, III, huit, acht]
          </p>
        </div>
      </div>
    </div>
    <!--/ Mechanistic interpretability evidence -->
  </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="container is-max-desktop"">
    <!-- Introduction. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Applications</h2>
        <div class="content has-text-justified">
          <p>
          <b>a) Understand the mechanism of LoRA.</b>
          </p>
          <p>
          We apply the comparable neuron analysis method to explore the mechanism 
          of LoRA. We add LoRA on the 9th attention layer and compare the important neurons between the original model 
          and the LoRA model, and we find that LoRA enhances the accuracy by enlarging the important neurons’ coefficient 
          scores. In other words, the deep FFN neurons can be regarded as learned features. LoRA learns how to enlarge 
          the features, rather than learning the features in the LoRA parameters.
          </p>
          <p>
          <b>b) Model pruning for arithmetic tasks.</b>
          </p>
          <p>
          In previous analysis, we find that the accuracy is not affected much when 
          pruning many deep neurons. Based on this, we propose a method for model pruning. We first use comparable neuron 
          analysis method to locate the important neurons by comparing the original model with the LoRA fine-tuned model, 
          and get the pruned model by pruning the neurons in deep layers which are not important. Then we add another LoRA 
          and fine-tune it again on the pruned model. This method can get 82.3% accuracy when pruning 95% deep FFN neurons.
          When randomly prune the same neurons and add LoRA on this randomly-pruned model, the accuracy is only 17.1%.
          </p>
          <p>
          <b>c) Model editing for reducing gender bias.</b>
          </p>
          <p>
          We compare the neuron’s importance score under different input cases 
          (e.g. A woman works as a -> nurse & A man works as a -> nurse). The neurons’ importance 
          scores are different under these cases, because the probability is not the same. Using this method, we can 
          locate the gender bias neurons and reduce the gender bias by zero-editing them. The average perplexity difference
          between different genders decreases 35.7% when only 18 neurons are edited.
          </p>
        </div>
        <br/>
      </div>
    </div>
    <!--/ Introduction. -->
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{yu2024neuron,
  author    = {Yu, Zeping and Ananiadou, Sophia},
  title     = {Interpreting Arithmetic Mechanism in Large Language Model through Comparative Neuron Analysis},
  journal   = {EMNLP},
  year      = {2024},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/zepingyu0512" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-6">
        <div class="content">
          <p>
            This website is created using <a rel="license"
                                              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
            under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
